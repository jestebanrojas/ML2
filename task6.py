#----------------------------------------------------------------------#
#----------------------------------------------------------------------#
## 6. Now, apply dimensionality reduction using all your algorithms to train the model with only 2 features per image.
# - Plot the 2 new features generated by your algorithm
# - Does this somehow impact the performance of your model?
#----------------------------------------------------------------------#
#----------------------------------------------------------------------#

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report
from unsupervised.dim_red import svd
from unsupervised.dim_red import pca
from unsupervised.dim_red import t_sne
from sklearn.preprocessing import StandardScaler



# Load MNIST dataset
mnist = fetch_openml('mnist_784', version=1,parser='auto')
X, y = mnist.data.astype('float32'), mnist.target.astype('int')

# Keep only digits 0 and 8
indices = np.logical_or(y == 0, y == 8)
X = X[indices]
y = y[indices]


    #----------------------------------------------------------------------#
    # --- SVD
    #----------------------------------------------------------------------#

# quantity of singular values to be considered
n_componentes = 2

# create an object from svd
mnist_svd = svd.SVD(n_components=n_componentes)
# Apply the svd transformation
X_transformed = mnist_svd.fit_transform(X)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n SVD 2 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)

    #----------------------------------------------------------------------#
    # --- SVD
    #----------------------------------------------------------------------#

# quantity of singular values to be considered
n_componentes = 4

# create an object from svd
mnist_svd = svd.SVD(n_components=n_componentes)

# Apply the svd transformation
X_transformed = mnist_svd.fit_transform(X)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n SVD 4 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)


    #----------------------------------------------------------------------#
    # --- PCA
    #----------------------------------------------------------------------#


    # quantity of singular values to be considered
n_componentes = 2

# create an object from PCA
mnist_pca = pca.PCA(n_components=n_componentes)


# fit the data
mnist_pca.fit(X)
# transform the data using the PCA object
X_transformed = mnist_pca.transform(X)




# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n PCA 2 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)


    #----------------------------------------------------------------------#
    # --- PCA
    #----------------------------------------------------------------------#


# quantity of singular values to be considered
n_componentes = 4

# create an object from PCA
mnist_pca = pca.PCA(n_components=n_componentes)


# fit the data
mnist_pca.fit(X)
# transform the data using the PCA object
X_transformed = mnist_pca.transform(X)




# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n PCA 4 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)


    #----------------------------------------------------------------------#
    # --- T-SNE
    #----------------------------------------------------------------------#


# quantity of singular values to be considered
n_componentes = 2


#Reduce the number of samples to reduce the processing time
subset_size = 5000
subset_indices = np.random.choice(len(X), size=subset_size, replace=False)
X, y = X.iloc[subset_indices], y.iloc[subset_indices]

X1=np.array(X)

#Normalize data. I've dcided not to use it, there was not significant improvement.
#scaler = StandardScaler()
#X_normalized = scaler.fit_transform(X1)

tsne = t_sne.TSNE(n_components=n_componentes, max_iter=1000)
X_transformed = tsne.fit_transform(X1)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n T-SNE 2 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)


    #----------------------------------------------------------------------#
    # --- T-SNE
    #----------------------------------------------------------------------#

# quantity of singular values to be considered
n_componentes = 4


#Reduce the number of samples to reduce the processing time
subset_size = 5000
subset_indices = np.random.choice(len(X), size=subset_size, replace=False)
X, y = X.iloc[subset_indices], y.iloc[subset_indices]

X1=np.array(X)

#Normalize data. I've dcided not to use it, there was not significant improvement.
#scaler = StandardScaler()
#X_normalized = scaler.fit_transform(X1)

tsne = t_sne.TSNE(n_components=n_componentes, max_iter=1000)
X_transformed = tsne.fit_transform(X1)


# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
print('inicio')
# Train a naive logistic regression model
model = LogisticRegression(solver='liblinear', max_iter=100)
model.fit(X_train, y_train)
# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print('\n T-SNE 4 features')
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred)
print('\nClassification Report:\n', report)